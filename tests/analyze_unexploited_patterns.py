"""
Empirical analysis to find patterns in SINQ that existing methods don't exploit.

Goal: Find something UNIQUE about the Sinkhorn + quantization combination
that hasn't been covered by prior work.

Prior work covers:
- Importance-based pruning (Wanda, OWQ)
- Joint optimization (JSQ, OBR, AWP)
- Structural sparsity (GQSA, QPruner)
- Order of operations (ICLR 2025)

What's unique to SINQ?
1. Sinkhorn creates μ factors encoding row/column scale
2. Dual scale-shift quantization
3. The INTERACTION between μ factors and quantization
"""

import sys
sys.path.insert(0, '/workspace/SINQ')

import torch
import numpy as np
from transformers import AutoModelForCausalLM
from sinq.sinkhorn import sinkhorn_log
from sinq.dual_shift import quantize_rtn


def analyze_layer(name, W):
    print(f"\n{'='*70}")
    print(f"Layer: {name}, Shape: {W.shape}")
    print("="*70)

    K, N = W.shape
    group_size = 64
    n_groups = N // group_size
    nbits = 4
    min_max = [0, 2**nbits - 1]

    # Sinkhorn
    W_norm, mu1, mu2 = sinkhorn_log(W, order=16)
    mu2 = mu2.squeeze()

    print("\n--- μ Factor Analysis ---")
    print(f"μ₁ (column): mean={mu1.mean():.4f}, std={mu1.std():.4f}, CV={mu1.std()/mu1.mean():.4f}")
    print(f"μ₂ (row):    mean={mu2.mean():.4f}, std={mu2.std():.4f}, CV={mu2.std()/mu2.mean():.4f}")

    # Quantize normalized weights
    Q, scales, zeros, _ = quantize_rtn(W_norm, min_max, group_size=group_size)
    Q_grouped = Q.view(K, n_groups, group_size)
    W_deq_norm = (Q_grouped - zeros) * scales
    W_deq_norm = W_deq_norm.view(K, N)

    # Per-weight quantization error
    quant_error = (W_norm - W_deq_norm).abs()

    print("\n--- Quantization Error Analysis ---")
    print(f"Quant error: mean={quant_error.mean():.6f}, std={quant_error.std():.6f}")

    # KEY ANALYSIS: Does μ predict quantization error?
    mu_product = mu1.unsqueeze(0) * mu2.unsqueeze(1)  # [K, N]

    corr_mu_qerr = torch.corrcoef(torch.stack([
        mu_product.flatten(),
        quant_error.flatten()
    ]))[0, 1].item()
    print(f"Correlation(μ product, quant_error): {corr_mu_qerr:.4f}")

    # Does high μ correlate with high weight magnitude?
    corr_mu_mag = torch.corrcoef(torch.stack([
        mu_product.flatten(),
        W.abs().flatten()
    ]))[0, 1].item()
    print(f"Correlation(μ product, |W|): {corr_mu_mag:.4f}")

    # KEY ANALYSIS: Per-group scale variation
    scales_flat = scales.squeeze()  # [K, n_groups]
    print(f"\n--- Per-Group Scale Analysis ---")
    print(f"Scales: mean={scales_flat.mean():.6f}, std={scales_flat.std():.6f}, CV={scales_flat.std()/scales_flat.mean():.4f}")

    # Does μ₂ (row factor) correlate with average scale per row?
    scale_per_row = scales_flat.mean(dim=1)  # [K]
    corr_mu2_scale = torch.corrcoef(torch.stack([
        mu2.flatten(),
        scale_per_row.flatten()
    ]))[0, 1].item()
    print(f"Correlation(μ₂, avg_scale_per_row): {corr_mu2_scale:.4f}")

    # KEY ANALYSIS: What about the SPREAD of scales within a row?
    scale_std_per_row = scales_flat.std(dim=1)  # [K]
    corr_mu2_scale_spread = torch.corrcoef(torch.stack([
        mu2.flatten(),
        scale_std_per_row.flatten()
    ]))[0, 1].item()
    print(f"Correlation(μ₂, scale_std_per_row): {corr_mu2_scale_spread:.4f}")

    # KEY ANALYSIS: Is there a pattern in which weights get high quant error?
    # Are they clustered in certain rows/columns?
    high_error_threshold = quant_error.mean() + 2 * quant_error.std()
    high_error_mask = (quant_error > high_error_threshold)
    high_error_per_row = high_error_mask.float().mean(dim=1)  # [K]
    high_error_per_col = high_error_mask.float().mean(dim=0)  # [N]

    print(f"\n--- High Quant Error Distribution ---")
    print(f"High error threshold: {high_error_threshold:.6f}")
    print(f"Fraction of high-error weights: {high_error_mask.float().mean():.4f}")
    print(f"Per-row high error: mean={high_error_per_row.mean():.4f}, std={high_error_per_row.std():.4f}")
    print(f"Per-col high error: mean={high_error_per_col.mean():.4f}, std={high_error_per_col.std():.4f}")

    # Does μ₂ correlate with high-error-per-row?
    corr_mu2_higherr = torch.corrcoef(torch.stack([
        mu2.flatten(),
        high_error_per_row.flatten()
    ]))[0, 1].item()
    print(f"Correlation(μ₂, high_error_per_row): {corr_mu2_higherr:.4f}")

    # KEY ANALYSIS: Dequantized value in original space
    W_deq = W_deq_norm * mu2.unsqueeze(1) * mu1.unsqueeze(0)
    total_error = (W - W_deq).abs()
    print(f"\n--- Total Reconstruction Error (original space) ---")
    print(f"Total error: mean={total_error.mean():.6f}, std={total_error.std():.6f}")

    # Does μ amplify or dampen the error?
    error_amplification = total_error / (quant_error + 1e-8)
    print(f"Error amplification by μ: mean={error_amplification.mean():.2f}, std={error_amplification.std():.2f}")

    # KEY ANALYSIS: The μ factors change the effective importance of quant error
    # High μ → quant error is amplified → these weights need careful handling
    # Low μ → quant error is dampened → these weights are more forgiving

    # Which weights have HIGH quant error AND high μ (double bad)?
    double_bad = (quant_error > quant_error.median()) & (mu_product > mu_product.median())
    print(f"\n--- Double Bad Analysis ---")
    print(f"Weights with high quant error AND high μ: {double_bad.float().mean():.4f}")

    # Which weights have LOW quant error AND low μ (double good)?
    double_good = (quant_error < quant_error.median()) & (mu_product < mu_product.median())
    print(f"Weights with low quant error AND low μ: {double_good.float().mean():.4f}")

    # KEY INSIGHT: Is there UNEXPLOITED structure in the quant error distribution?
    # Group-level quant error analysis
    quant_error_grouped = quant_error.view(K, n_groups, group_size)
    quant_error_per_group = quant_error_grouped.mean(dim=2)  # [K, n_groups]
    print(f"\n--- Per-Group Quant Error Variation ---")
    print(f"Per-group error: mean={quant_error_per_group.mean():.6f}")
    print(f"Between-group std: {quant_error_per_group.std():.6f}")
    print(f"Within-group std avg: {quant_error_grouped.std(dim=2).mean():.6f}")

    # Groups with highest quant error - what's special about them?
    flat_group_error = quant_error_per_group.flatten()
    _, top_error_groups = flat_group_error.topk(10)

    print(f"\n--- Top 10 High-Error Groups ---")
    for idx in top_error_groups[:5]:
        row = idx // n_groups
        group = idx % n_groups
        group_weights = W_norm[row, group*group_size:(group+1)*group_size]
        group_scale = scales_flat[row, group].item()
        group_mu2 = mu2[row].item()
        print(f"  Row {row}, Group {group}: scale={group_scale:.6f}, μ₂={group_mu2:.4f}, "
              f"W range=[{group_weights.min():.4f}, {group_weights.max():.4f}]")


def main():
    print("="*70)
    print("UNEXPLOITED PATTERN ANALYSIS")
    print("="*70)

    torch.manual_seed(42)

    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-0.5B", torch_dtype=torch.float16, device_map="auto"
    )

    # Analyze multiple layers
    layers = [
        ('L0.gate_proj', model.model.layers[0].mlp.gate_proj.weight.data.float()),
        ('L0.q_proj', model.model.layers[0].self_attn.q_proj.weight.data.float()),
        ('L10.gate_proj', model.model.layers[10].mlp.gate_proj.weight.data.float()),
    ]

    for name, W in layers:
        analyze_layer(name, W)

    del model
    torch.cuda.empty_cache()


if __name__ == '__main__':
    main()
